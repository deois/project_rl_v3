"""
ë°±í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì
í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„
"""

import threading
import time
from typing import Callable, Optional, Dict, Any, List, Tuple
import numpy as np
import torch

from src.utils.logger import get_logger
from src.ddpg_algorithm import DDPGAgent
from src.data.merge import load_merged_data_v1
from src.environment.trading_env import TradingEnvironment


class DashBacktestManager:
    """Dashìš© ë°±í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì"""

    def __init__(
        self,
        log_callback: Optional[Callable[[str], None]] = None,
        result_callback: Optional[Callable[..., None]] = None,
        progress_callback: Optional[Callable[[int, int, float, str], None]] = None,
    ):
        self.log_callback = log_callback
        self.result_callback = result_callback  # ê²°ê³¼ ì½œë°± ì¶”ê°€
        self.progress_callback = progress_callback  # ì§„í–‰ë¥  ì½œë°± ì¶”ê°€
        self.logger = get_logger("dash_backtest")
        self.is_running = False

    def add_log(self, message: str):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶”ê°€"""
        if self.log_callback:
            self.log_callback(message)
        self.logger.info(message)

    def update_progress(
        self, current_step: int, total_steps: int, status: str = "ì§„í–‰ ì¤‘"
    ):
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        progress_percent = (
            min((current_step / total_steps) * 100, 100) if total_steps > 0 else 0
        )
        if self.progress_callback:
            self.progress_callback(current_step, total_steps, progress_percent, status)

    def update_results(
        self,
        portfolio_values: List[float],
        rewards: List[float],
        dates: List[str],
        allocations: List[Dict[str, float]],
        metrics: Dict[str, Any],
        additional_data: Optional[Dict[str, Any]] = None,
    ):
        """ê²°ê³¼ ì—…ë°ì´íŠ¸"""
        if self.result_callback:
            # ë°ì´í„°ê°€ ì œëŒ€ë¡œ ì „ë‹¬ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œê·¸
            self.add_log(
                f"ğŸ“¤ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì½œë°± ì „ì†¡: í¬íŠ¸í´ë¦¬ì˜¤={len(portfolio_values)}, ë°°ë¶„={len(allocations)}"
            )
            # ì¶”ê°€ ë°ì´í„°ë„ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
            self.result_callback(
                portfolio_values, rewards, dates, allocations, metrics, additional_data
            )

    def start_backtest(self, config: Dict[str, Any]) -> bool:
        """ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘"""

        if self.is_running:
            self.add_log("âš ï¸ ì´ë¯¸ ë°±í…ŒìŠ¤íŠ¸ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return False

        self.is_running = True

        # ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        backtest_thread = threading.Thread(
            target=self._backtest_worker, args=(config,), daemon=True
        )
        backtest_thread.start()

        return True

    def _backtest_worker(self, config: Dict[str, Any]):
        """ë°±í…ŒìŠ¤íŠ¸ ì›Œì»¤"""

        try:
            self.add_log(
                f"ğŸ“ˆ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘: {config['model_path']}, ì—í”¼ì†Œë“œ {config['episode']}"
            )

            # ì¥ì¹˜ ì„¤ì •
            device = self._get_device()
            self.add_log(f"ğŸ”§ ì—°ì‚° ì¥ì¹˜: {device}")

            # ëª¨ë¸ ì„¤ì • ë¡œë“œ ë° í™˜ê²½ êµ¬ì„±
            env, agent = self._setup_backtest_environment(config, device)

            # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            self.add_log("ğŸš€ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            results = self._run_evaluation(env, agent, config["assets"])

            # ê²°ê³¼ ì²˜ë¦¬ ë° ì½œë°± ì „ì†¡
            self._process_backtest_results(results)

            # ë°±í…ŒìŠ¤íŠ¸ ì„±ê³µ ì™„ë£Œ ì‹œ ì§„í–‰ë¥  ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
            self.update_progress(100, 100, "ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

        except Exception as e:
            error_message = f"âŒ ë°±í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.add_log(error_message)
            self.logger.error(error_message, exc_info=True)
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì§„í–‰ë¥  ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_progress(0, 1, "ì˜¤ë¥˜ ë°œìƒ")
        finally:
            self.is_running = False

    def _get_device(self):
        """ìµœì  ì¥ì¹˜ ì„ íƒ"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            device_name = "CUDA GPU"
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            device_name = "Apple Silicon GPU (MPS)"
        else:
            device = torch.device("cpu")
            device_name = "CPU"

        return device

    def _setup_backtest_environment(
        self, config: Dict[str, Any], device
    ) -> Tuple[TradingEnvironment, DDPGAgent]:
        """ë°±í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""

        # ğŸ”§ ë©”íƒ€ë°ì´í„° ë¨¼ì € ì½ì–´ì„œ ëª¨ë¸ ì„¤ì • íŒŒì•…
        self.add_log("ğŸ“‹ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì½ëŠ” ì¤‘...")
        from src.dash_utils import (
            get_model_metadata,
            load_model_training_config,
            get_latest_episode_from_model,
        )

        model_metadata = get_model_metadata(config["model_path"])
        training_config = load_model_training_config(config["model_path"])

        # ì—í”¼ì†Œë“œê°€ 0ì¸ ê²½ìš° ìµœì‹  ì—í”¼ì†Œë“œ ìë™ ê°ì§€
        if config["episode"] == 0 or config["episode"] is None:
            latest_episode = get_latest_episode_from_model(config["model_path"])
            if latest_episode > 0:
                config["episode"] = latest_episode
                self.add_log(f"ğŸ” ìµœì‹  ì—í”¼ì†Œë“œ ìë™ ê°ì§€: {latest_episode}")
            else:
                self.add_log(
                    "âš ï¸ ìœ íš¨í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤"
                )
                config["episode"] = None

        if not training_config:
            # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
            self.add_log("âš ï¸ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            model_settings = {
                "hidden_dim": 256,
                "actor_lr": 0.0003,
                "critic_lr": 0.0003,
                "critic_loss_type": "mse",
                "window_size": 60,
            }
        else:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ëª¨ë¸ ì„¤ì • ì¶”ì¶œ
            model_settings = {
                "hidden_dim": training_config.get("hidden_dim", 256),
                "actor_lr": training_config.get("actor_lr", 0.0003),
                "critic_lr": training_config.get("critic_lr", 0.0003),
                "critic_loss_type": training_config.get("critic_loss_type", "mse"),
                "window_size": training_config.get("window_size", 60),
            }

        self.add_log(
            f"ğŸ“Š ëª¨ë¸ ì„¤ì • í™•ì¸: íˆë“ ì°¨ì›={model_settings['hidden_dim']}, "
            f"ìœˆë„ìš°í¬ê¸°={model_settings['window_size']}, "
            f"Actor LR={model_settings['actor_lr']}, Critic LR={model_settings['critic_lr']}"
        )

        # ëª¨ë¸ì˜ ETF ì •ë³´ í™•ì¸ ë° ì‚¬ìš©
        model_etfs = training_config.get("assets", []) if training_config else []
        if model_etfs:
            self.add_log(f"ğŸ“Š ëª¨ë¸ í•™ìŠµ ETF: {', '.join(model_etfs)}")
            # ëª¨ë¸ì´ í•™ìŠµëœ ETF ì‚¬ìš©
            config["assets"] = model_etfs
        else:
            self.add_log(f"ğŸ“Š ì„¤ì •ëœ ETF ì‚¬ìš©: {', '.join(config['assets'])}")

        # ë°ì´í„° ë¡œë“œ (ETF ì¡°í•©ì— ë”°ë¥¸ íŒŒì¼ëª… ìƒì„±)
        self.add_log(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘... (ETF: {', '.join(config['assets'])})")
        etf_combination = "_".join(sorted(config["assets"]))
        filename = f"rl_ddpg_{etf_combination}"
        merged_data = load_merged_data_v1(config["assets"], filename, refresh=False)
        self.add_log(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(merged_data)} í–‰)")

        # í™˜ê²½ ì„¤ì • (ë©”íƒ€ë°ì´í„°ì˜ ìœˆë„ìš° í¬ê¸° ì‚¬ìš©)
        self.add_log("ğŸ—ï¸ íŠ¸ë ˆì´ë”© í™˜ê²½ ì„¤ì • ì¤‘...")
        env = TradingEnvironment(
            merged_data,
            self.logger,
            window_size=int(model_settings["window_size"]),
            n_assets=len(config["assets"]),
        )

        # ì—ì´ì „íŠ¸ ì„¤ì • (ë©”íƒ€ë°ì´í„°ì˜ ëª¨ë¸ ì„¤ì • ì‚¬ìš©)
        self.add_log("ğŸ¤– DDPG ì—ì´ì „íŠ¸ ìƒì„± ì¤‘ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì„¤ì •)...")
        state_dim = (
            env.observation_space.shape[0] if env.observation_space is not None else 0
        )
        action_dim = env.action_space.shape[0] if env.action_space is not None else 0

        agent_ddpg = DDPGAgent(
            self.logger,
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=int(model_settings["hidden_dim"]),
            actor_lr=model_settings["actor_lr"],
            critic_lr=model_settings["critic_lr"],
            device=str(device),
            critic_loss_type=model_settings.get("critic_loss_type", "mse"),
        )

        # ëª¨ë¸ ë¡œë“œ
        episode_to_load = config["episode"]
        if episode_to_load is None:
            self.add_log(f"ğŸ“¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {config['model_path']}")
        else:
            self.add_log(
                f"ğŸ“¥ ì—í”¼ì†Œë“œ {episode_to_load} ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {config['model_path']}"
            )

        ret = agent_ddpg.load_checkpoint(
            config["model_path"], episode_to_load, evaluate=True
        )

        if ret:
            # ë¡œë“œ ì„±ê³µ ì‹œ ì‹¤ì œ ì—í”¼ì†Œë“œ ë²ˆí˜¸ í™•ì¸ ë° ë¡œê·¸
            metadata_info = get_model_metadata(config["model_path"])
            if metadata_info:
                actual_episode = metadata_info["episode"]
                self.add_log(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: ì—í”¼ì†Œë“œ {actual_episode}")
            else:
                self.add_log(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
        else:
            self.add_log("âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨")
            raise Exception("ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # í‰ê°€ ëª¨ë“œ ì¶”ê°€ ì„¤ì •
        agent_ddpg.reset_for_evaluation()
        self.add_log("âœ… ëª¨ë¸ ë¡œë“œ ë° í‰ê°€ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")

        return env, agent_ddpg

    def _process_backtest_results(self, results: Dict[str, Any]):
        """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì²˜ë¦¬ ë° ì½œë°± ì „ì†¡"""

        # ê²°ê³¼ë¥¼ ì½œë°±ìœ¼ë¡œ ì „ë‹¬ - ê°•í™”í•™ìŠµê³¼ ê· ë“±íˆ¬ì ë¹„êµ ë°ì´í„°
        if results and results.get("RL_Agent") and results.get("Equal_Weight"):
            rl_results = results["RL_Agent"]
            equal_results = results["Equal_Weight"]

            # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶œë ¥
            rl_allocations = rl_results.get("allocations", [])
            equal_allocations = equal_results.get("allocations", [])

            # self.add_log(f"ğŸ“Š ê°•í™”í•™ìŠµ ë°°ë¶„ ë°ì´í„°: {len(rl_allocations)}ê°œ í•­ëª©")
            # self.add_log(f"ğŸ“Š ê· ë“±íˆ¬ì ë°°ë¶„ ë°ì´í„°: {len(equal_allocations)}ê°œ í•­ëª©")

            # if rl_allocations and len(rl_allocations) > 0:
            #     self.add_log(f"ğŸ“Š ê°•í™”í•™ìŠµ ì²« ë°°ë¶„: {rl_allocations[0]}")
            #     self.add_log(f"ğŸ“Š ê°•í™”í•™ìŠµ ë§ˆì§€ë§‰ ë°°ë¶„: {rl_allocations[-1]}")

            # if equal_allocations and len(equal_allocations) > 0:
            #     self.add_log(f"ğŸ“Š ê· ë“±íˆ¬ì ì²« ë°°ë¶„: {equal_allocations[0]}")

            # ë¹„êµ ë°ì´í„° êµ¬ì„± (ë‘ ì „ëµ ëª¨ë‘ í¬í•¨)
            comparison_data = {
                "rl_strategy": {
                    "portfolio_values": rl_results.get("portfolio_values", []),
                    "rewards": rl_results.get("rewards", []),
                    "dates": rl_results.get("dates", []),
                    "allocations": rl_allocations,
                    "allocation_dates": rl_results.get("allocation_dates", []),
                    "annualized_returns": rl_results.get("annualized_returns", []),
                    "cumulative_returns": rl_results.get("cumulative_returns", []),
                },
                "equal_strategy": {
                    "portfolio_values": equal_results.get("portfolio_values", []),
                    "rewards": equal_results.get("rewards", []),
                    "dates": equal_results.get("dates", []),
                    "allocations": equal_allocations,
                    "allocation_dates": equal_results.get("allocation_dates", []),
                    "annualized_returns": equal_results.get("annualized_returns", []),
                    "cumulative_returns": equal_results.get("cumulative_returns", []),
                },
            }

            # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ê°•í™”í•™ìŠµ ë°ì´í„°ë¥¼ ë©”ì¸ìœ¼ë¡œ ì „ë‹¬í•˜ë˜, ì¶”ê°€ ë°ì´í„°ì— ë¹„êµ ì •ë³´ í¬í•¨
            self.update_results(
                rl_results.get("portfolio_values", []),
                rl_results.get("rewards", []),
                rl_results.get("dates", []),
                rl_allocations,
                results.get("final_metrics", {}),
                comparison_data,
            )

        # # ê²°ê³¼ ìš”ì•½
        # final_metrics = results.get("final_metrics", {})
        # summary = (
        #     f"ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ! "
        #     f"ìµœì¢… ìˆ˜ìµë¥ : {final_metrics.get('total_return', 0):.2f}%, "
        #     f"ì—°í™˜ì‚° ìˆ˜ìµë¥ : {final_metrics.get('annualized_return', 0):.2f}%, "
        #     f"ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤: ${final_metrics.get('final_portfolio_value', 0):.2f}"
        # )
        # self.add_log(summary)

    def _run_evaluation(self, env, agent, assets: List[str]) -> Dict[str, Any]:
        """ì‹¤ì œ í‰ê°€ ë¡œì§ - ê°•í™”í•™ìŠµ ëª¨ë¸ê³¼ ê· ë“±íˆ¬ì ì „ëµ ë¹„êµ"""

        results = {
            "RL_Agent": {  # ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ê²°ê³¼
                "portfolio_values": [],
                "rewards": [],
                "dates": [],
                "allocations": [],
                "allocation_dates": [],  # ë°°ë¶„ ë‚ ì§œ ë³„ë„ ì €ì¥
                "annualized_returns": [],
                "cumulative_returns": [],
            },
            "Equal_Weight": {  # ê· ë“±íˆ¬ì ê²°ê³¼
                "portfolio_values": [],
                "rewards": [],
                "dates": [],
                "allocations": [],
                "allocation_dates": [],  # ë°°ë¶„ ë‚ ì§œ ë³„ë„ ì €ì¥
                "annualized_returns": [],
                "cumulative_returns": [],
            },
            "final_metrics": {},
        }

        # í™˜ê²½ ì´ˆê¸°í™”
        state = env.reset()
        step_count = 0

        try:
            total_steps = len(env.data) - env.window_size
        except:
            total_steps = 1000  # ê¸°ë³¸ê°’

        self.add_log(f"ğŸ“ˆ í‰ê°€ ì‹œì‘ (ì˜ˆìƒ ìŠ¤í…: {total_steps})")
        self.add_log(f"ğŸ†š ê°•í™”í•™ìŠµ vs ê· ë“±íˆ¬ì ì „ëµ ë¹„êµ ë¶„ì„")

        # ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œ ì§„í–‰ë¥  ì´ˆê¸°í™”
        self.update_progress(0, total_steps, "ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # ì—í”¼ì†Œë“œ ë³´ìƒ ì¶”ì 
        rl_episode_reward = 0
        equal_episode_reward = 0

        while True:
            # ì§„í–‰ë¥  ë¡œê·¸ ë° UI ì—…ë°ì´íŠ¸ (ë” ìì£¼ ì—…ë°ì´íŠ¸)
            if step_count % 50 == 0 and step_count > 0:  # 100ì—ì„œ 50ìœ¼ë¡œ ë³€ê²½
                progress = (step_count / total_steps) * 100
                self.add_log(
                    f"â³ ë°±í…ŒìŠ¤íŠ¸ ì§„í–‰: {step_count}/{total_steps} ({progress:.1f}%)"
                )
                self.update_progress(step_count, total_steps, "ë°±í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘")
                env.render()

            # íŠ¹ë³„ ì§„í–‰ë¥  ì²´í¬í¬ì¸íŠ¸ (10% ë‹¨ìœ„)
            progress_checkpoint = (step_count / total_steps) * 100
            if (
                step_count > 0
                and int(progress_checkpoint) % 10 == 0
                and int(progress_checkpoint)
                != int(((step_count - 1) / total_steps) * 100)
            ):
                self.add_log(
                    f"ğŸ¯ ë°±í…ŒìŠ¤íŠ¸ ì²´í¬í¬ì¸íŠ¸: {int(progress_checkpoint)}% ì™„ë£Œ"
                )
                self.update_progress(
                    step_count,
                    total_steps,
                    f"ë°±í…ŒìŠ¤íŠ¸ {int(progress_checkpoint)}% ì™„ë£Œ",
                )

            # ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ì•¡ì…˜ ì„ íƒ
            # select_actionì—ì„œ ì´ë¯¸ ìµœì†Œ ë¹„ì¤‘ 10% ì œì•½ì„ ë³´ì¥í•˜ë¯€ë¡œ ì¶”ê°€ í´ë¦¬í•‘ ë¶ˆí•„ìš”
            rl_action, raw_action = agent.select_action(state, add_noise=False)

            # ì•¡ì…˜ ìœ íš¨ì„± ê²€ì‚¬ (ìµœì†Œ ë¹„ì¤‘ ì œì•½ í™•ì¸)
            # select_actionì—ì„œ ì´ë¯¸ ìµœì†Œ ë¹„ì¤‘ 10%ë¥¼ ë³´ì¥í•˜ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ê²€ì¦
            min_weight = 0.075
            if np.min(rl_action) < min_weight:
                # ìµœì†Œ ë¹„ì¤‘ ë¯¸ë§Œì¸ ê²½ìš° ì¬ì¡°ì • (ì´ë¡ ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨)
                rl_action = np.maximum(rl_action, min_weight)
                rl_action = rl_action / np.sum(rl_action)

            # í•©ì´ 1ì¸ì§€ í™•ì¸ (ì´ë¡ ì ìœ¼ë¡œ ì´ë¯¸ ë³´ì¥ë¨)
            action_sum = np.sum(rl_action)
            if abs(action_sum - 1.0) > 1e-6:
                rl_action = rl_action / action_sum

            # í™˜ê²½ ìŠ¤í… ì‹¤í–‰
            (
                next_state,
                reward_agent,
                reward_monthly_agent,
                reward_monthly_equal,
                done,
                _,
                verification,
            ) = env.step(rl_action)

            # ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
            self._collect_evaluation_data(
                env,
                rl_action,
                assets,
                verification,
                reward_monthly_agent,
                reward_monthly_equal,
                results,
                rl_episode_reward,
                equal_episode_reward,
            )

            state = next_state
            step_count += 1

            if done:
                break

        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì„±ëŠ¥ ë¹„êµ ë¡œê·¸
        self._calculate_final_metrics(env, results, step_count, total_steps)

        return results

    def _collect_evaluation_data(
        self,
        env,
        rl_action,
        assets,
        verification,
        reward_monthly_agent,
        reward_monthly_equal,
        results,
        rl_episode_reward,
        equal_episode_reward,
    ):
        """í‰ê°€ ë°ì´í„° ìˆ˜ì§‘"""

        # ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ë°ì´í„° ìˆ˜ì§‘
        current_rl_value = env._calculate_value()
        current_date_str, current_date_obj = env._current_date()  # íŠœí”Œ ì–¸íŒ¨í‚¹

        # ê· ë“±íˆ¬ì ì „ëµ ë°ì´í„° ìˆ˜ì§‘
        current_equal_value = (
            np.sum(
                env.shares_equal
                * env.original_data.iloc[env.current_step][
                    env.original_data.columns[: env.n_assets]
                ].values
            )
            + env.balance_equal
        )

        # ì—°í™˜ì‚° ìˆ˜ìµë¥ ê³¼ ì´ ìˆ˜ìµë¥  ê³„ì‚° (í™˜ê²½ ê°ì²´ì˜ ë©”ì„œë“œ ì‚¬ìš©)
        rl_annualized_return = env._calculate_annualized_return()
        rl_total_return = env._calculate_total_return()

        # ê· ë“±íˆ¬ì ì „ëµì„ ìœ„í•œ ë³„ë„ ìˆ˜ìµë¥  ê³„ì‚°
        # ì„ì‹œë¡œ í™˜ê²½ì˜ ìƒíƒœë¥¼ ê· ë“±íˆ¬ì ê°’ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ê³„ì‚°
        original_balance = env.balance
        original_shares = env.shares.copy()
        original_total_invested = env.total_invested

        # ê· ë“±íˆ¬ì ì „ëµì˜ ê°’ìœ¼ë¡œ ì„ì‹œ ë³€ê²½
        env.balance = env.balance_equal
        env.shares = env.shares_equal.copy()
        env.total_invested = env.total_invested  # ì´ íˆ¬ìê¸ˆì€ ë™ì¼
        equal_annualized_return = env._calculate_annualized_return()
        equal_total_return = env._calculate_total_return()

        # ì›ë˜ ìƒíƒœë¡œ ë³µì›
        env.balance = original_balance
        env.shares = original_shares
        env.total_invested = original_total_invested

        # ë°ì´í„° ì €ì¥ (ë‚ ì§œëŠ” ë¬¸ìì—´ í˜•íƒœë¡œ ì €ì¥)
        results["RL_Agent"]["portfolio_values"].append(current_rl_value)
        results["RL_Agent"]["dates"].append(current_date_str)
        results["RL_Agent"]["rewards"].append(
            reward_monthly_agent if verification else 0
        )
        results["RL_Agent"]["annualized_returns"].append(rl_annualized_return)
        results["RL_Agent"]["cumulative_returns"].append(rl_total_return)

        results["Equal_Weight"]["portfolio_values"].append(current_equal_value)
        results["Equal_Weight"]["dates"].append(current_date_str)
        results["Equal_Weight"]["rewards"].append(
            reward_monthly_equal if verification else 0
        )
        results["Equal_Weight"]["annualized_returns"].append(equal_annualized_return)
        results["Equal_Weight"]["cumulative_returns"].append(equal_total_return)

        # í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„ ì €ì¥ (ë¦¬ë°¸ëŸ°ì‹± ì‹œì ë§Œ)
        if verification:
            # ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ë°°ë¶„
            rl_allocation = {}
            for i, asset in enumerate(assets):
                rl_allocation[asset] = float(rl_action[i])
            rl_allocation["Cash"] = float(rl_action[-1])
            results["RL_Agent"]["allocations"].append(rl_allocation)
            # ë°°ë¶„ ë‚ ì§œë„ ë³„ë„ ì €ì¥
            results["RL_Agent"]["allocation_dates"].append(current_date_str)

            # ê· ë“±íˆ¬ì ì „ëµ ë°°ë¶„ (25%ì”© ê· ë“±ë¶„ë°°, í˜„ê¸ˆ 0%)
            equal_allocation = {}
            for asset in assets:
                equal_allocation[asset] = 0.25  # 4ê°œ ìì‚°ì— 25%ì”©
            equal_allocation["Cash"] = 0.0
            results["Equal_Weight"]["allocations"].append(equal_allocation)
            # ë°°ë¶„ ë‚ ì§œë„ ë³„ë„ ì €ì¥
            results["Equal_Weight"]["allocation_dates"].append(current_date_str)

            # ë³´ìƒ ëˆ„ì 
            rl_episode_reward += reward_monthly_agent
            equal_episode_reward += reward_monthly_equal

    def _calculate_final_metrics(self, env, results, step_count, total_steps):
        """ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì„±ëŠ¥ ë¹„êµ"""

        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        rl_final_value = (
            results["RL_Agent"]["portfolio_values"][-1]
            if results["RL_Agent"]["portfolio_values"]
            else env.initial_balance
        )
        equal_final_value = (
            results["Equal_Weight"]["portfolio_values"][-1]
            if results["Equal_Weight"]["portfolio_values"]
            else env.initial_balance
        )

        # ì´ íˆ¬ì ê¸ˆì•¡
        total_invested = env.total_invested

        # ìˆ˜ìµë¥  ê³„ì‚°
        rl_total_return = (
            ((rl_final_value - total_invested) / total_invested) * 100
            if total_invested > 0
            else 0
        )
        equal_total_return = (
            ((equal_final_value - total_invested) / total_invested) * 100
            if total_invested > 0
            else 0
        )

        # ì—°í™˜ì‚° ìˆ˜ìµë¥  ê³„ì‚° (ë‹¨ìˆœí™”)
        days_elapsed = len(results["RL_Agent"]["portfolio_values"])
        years_elapsed = days_elapsed / 365.0 if days_elapsed > 0 else 1

        rl_annualized_return = (
            (((rl_final_value / total_invested) ** (1 / years_elapsed)) - 1) * 100
            if total_invested > 0 and years_elapsed > 0
            else 0
        )
        equal_annualized_return = (
            (((equal_final_value / total_invested) ** (1 / years_elapsed)) - 1) * 100
            if total_invested > 0 and years_elapsed > 0
            else 0
        )

        # ìˆ˜ìµë¥  ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥
        for strategy in ["RL_Agent", "Equal_Weight"]:
            portfolio_values = results[strategy]["portfolio_values"]
            cumulative_returns = results[strategy]["cumulative_returns"]
            annualized_returns = results[strategy]["annualized_returns"]

            if portfolio_values and len(portfolio_values) > 0:
                # ì²« ë²ˆì§¸ í¬íŠ¸í´ë¦¬ì˜¤ ê°’ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©
                initial_value = portfolio_values[0]
                final_value = portfolio_values[-1]

                # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶œë ¥
                self.add_log(f"ğŸ” {strategy} ë””ë²„ê¹…:")
                self.add_log(f"   - ì²« ë²ˆì§¸ ê°’: ${initial_value:.2f}")
                self.add_log(f"   - ìµœì¢… ê°’: ${final_value:.2f}")
                self.add_log(f"   - ì´ íˆ¬ìê¸ˆ: ${total_invested:.2f}")
                self.add_log(
                    f"   - ì²« ë²ˆì§¸ ê°’ ê¸°ì¤€ ìˆ˜ìµë¥ : {((final_value - initial_value) / initial_value) * 100:.2f}%"
                )
                self.add_log(
                    f"   - ì´ íˆ¬ìê¸ˆ ê¸°ì¤€ ìˆ˜ìµë¥ : {((final_value - total_invested) / total_invested) * 100:.2f}%"
                )

                # ë§ˆì§€ë§‰ ëª‡ ê°œ ê°’ ë¡œê·¸ ì¶œë ¥ (ì´ë¯¸ í™˜ê²½ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ì €ì¥ë¨)
                if len(cumulative_returns) > 5:
                    self.add_log(
                        f"   - ë§ˆì§€ë§‰ 5ê°œ ëˆ„ì  ìˆ˜ìµë¥ : {cumulative_returns[-5:]}"
                    )

                # ì—°í™˜ì‚° ìˆ˜ìµë¥  ë¡œê·¸ ì¶œë ¥ (ì´ë¯¸ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ì €ì¥ë¨)
                if len(annualized_returns) > 5:
                    self.add_log(
                        f"   - ë§ˆì§€ë§‰ 5ê°œ ì—°í™˜ì‚° ìˆ˜ìµë¥ : {annualized_returns[-5:]}"
                    )

        # ìµœì¢… ë©”íŠ¸ë¦­ ìš”ì•½
        results["final_metrics"] = {
            # ê°•í™”í•™ìŠµ ë©”íŠ¸ë¦­
            "rl_total_return": rl_total_return,
            "rl_annualized_return": rl_annualized_return,
            "rl_final_portfolio_value": rl_final_value,
            # ê· ë“±íˆ¬ì ë©”íŠ¸ë¦­
            "equal_total_return": equal_total_return,
            "equal_annualized_return": equal_annualized_return,
            "equal_final_portfolio_value": equal_final_value,
            # ê³µí†µ ë©”íŠ¸ë¦­
            "total_invested": total_invested,
            "total_steps": step_count,
            "evaluation_days": days_elapsed,
            # ì„±ëŠ¥ ë¹„êµ
            "return_difference": rl_total_return - equal_total_return,
            "annualized_return_difference": rl_annualized_return
            - equal_annualized_return,
            "value_difference": rl_final_value - equal_final_value,
        }

        # ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œ ì§„í–‰ë¥  100% ì—…ë°ì´íŠ¸
        self.update_progress(step_count, total_steps, "ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

        # ì„±ëŠ¥ ë¹„êµ ë¡œê·¸
        self.add_log("ğŸ“Š === ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ===")
        self.add_log(
            f"ğŸ¤– ê°•í™”í•™ìŠµ: ìµœì¢… ${rl_final_value:.2f}, ìˆ˜ìµë¥  {rl_total_return:.2f}%, ì—°í™˜ì‚° {rl_annualized_return:.2f}%"
        )
        self.add_log(
            f"âš–ï¸ ê· ë“±íˆ¬ì: ìµœì¢… ${equal_final_value:.2f}, ìˆ˜ìµë¥  {equal_total_return:.2f}%, ì—°í™˜ì‚° {equal_annualized_return:.2f}%"
        )
        self.add_log(
            f"ğŸ† ì„±ê³¼ ì°¨ì´: ${rl_final_value - equal_final_value:.2f} ({'ê°•í™”í•™ìŠµ ìš°ì„¸' if rl_final_value > equal_final_value else 'ê· ë“±íˆ¬ì ìš°ì„¸'})"
        )

        # ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ë¡œê·¸
        rl_dates = results["RL_Agent"]["dates"]
        equal_dates = results["Equal_Weight"]["dates"]
        self.add_log(
            f"ğŸ“… ìˆ˜ì§‘ëœ ë‚ ì§œ ë°ì´í„°: ê°•í™”í•™ìŠµ {len(rl_dates)}ê°œ, ê· ë“±íˆ¬ì {len(equal_dates)}ê°œ"
        )
        if rl_dates:
            self.add_log(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {rl_dates[0]} ~ {rl_dates[-1]}")
            # ì¤‘ê°„ ë‚ ì§œë“¤ë„ í™•ì¸
            if len(rl_dates) > 10:
                sample_dates = [
                    rl_dates[i]
                    for i in [
                        0,
                        len(rl_dates) // 4,
                        len(rl_dates) // 2,
                        len(rl_dates) * 3 // 4,
                        -1,
                    ]
                ]
                self.add_log(f"ğŸ“… ìƒ˜í”Œ ë‚ ì§œë“¤: {sample_dates}")
