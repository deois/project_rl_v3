"""
ê°•í™”í•™ìŠµ í›ˆë ¨ ê´€ë¦¬ì
ì‹¤ì œ DDPG í•™ìŠµ ë¡œì§ì„ Dash ì½œë°±ì—ì„œ ì‹¤í–‰
"""

import os
import threading
import time
import random
from collections import deque
from typing import Callable, Optional, Dict, Any, Tuple
import numpy as np
import torch

from src.utils.logger import get_logger
from src.ddpg_algorithm import DDPGAgent
from src.data.merge import load_merged_data_v1
from src.environment.trading_env import TradingEnvironment
from .model_utils import calculate_model_hash


class DashRealTrainingManager:
    """Dashìš© ì‹¤ì œ ê°•í™”í•™ìŠµ ê´€ë¦¬ì"""

    def __init__(
        self,
        log_callback: Optional[Callable[[str], None]] = None,
        status_callback: Optional[Callable[..., None]] = None,
    ):
        self.log_callback = log_callback
        self.status_callback = status_callback
        self.logger = get_logger("dash_real_training")

        # í•™ìŠµ ìƒíƒœ
        self.is_training = False
        self.should_stop = False
        self.current_episode = 0
        self.total_episodes = 0
        self.current_reward = 0.0
        self.average_reward = 0.0
        self.portfolio_value = 0.0
        self.task_id: Optional[str] = None

        # í•™ìŠµ ì»¨íŠ¸ë¡¤
        self.stop_event: Optional[threading.Event] = None
        self.training_thread: Optional[threading.Thread] = None

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.actor_loss = 0.0
        self.critic_loss = 0.0
        self.update_count = 0

        # í•™ìŠµ ì‹œê°„ ì¶”ì 
        self.training_start_time: Optional[float] = None
        self.total_training_time = 0.0
        self.episode_times = []

        # ìˆ˜ë™ ì €ì¥ì„ ìœ„í•œ í˜„ì¬ í•™ìŠµ ìƒíƒœ ì €ì¥
        self.current_agent = None
        self.current_config: Optional[Dict[str, Any]] = None
        self.model_dir: Optional[str] = None

    def add_log(self, message: str):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶”ê°€"""
        if self.log_callback:
            self.log_callback(message)
        self.logger.info(message)

    def update_status(self, **kwargs):
        """ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if self.status_callback:
            self.status_callback(**kwargs)

    def start_real_training(self, task_id: str, config: Dict[str, Any]) -> bool:
        """ì‹¤ì œ ê°•í™”í•™ìŠµ ì‹œì‘"""

        if self.is_training:
            self.add_log("âš ï¸ ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return False

        self.task_id = task_id
        self.is_training = True
        self.stop_event = threading.Event()

        # í•™ìŠµ ì‹œê°„ ì¶”ì  ì‹œì‘
        self.training_start_time = time.time()
        self.episode_times = []

        # ìƒˆ ìŠ¤ë ˆë“œì—ì„œ í•™ìŠµ ì‹¤í–‰
        self.training_thread = threading.Thread(
            target=self._training_thread_worker, args=(task_id, config), daemon=True
        )
        self.training_thread.start()

        return True

    def stop_training(self):
        """í•™ìŠµ ì¤‘ì§€"""
        if self.stop_event:
            self.stop_event.set()
            self.add_log(f"ğŸ›‘ í•™ìŠµ ì¤‘ì§€ ì‹ í˜¸ ì „ì†¡ (ID: {self.task_id})")

    def _training_thread_worker(self, task_id: str, config: Dict[str, Any]):
        """í•™ìŠµ ì›Œì»¤ ìŠ¤ë ˆë“œ"""

        try:
            self.add_log(f"ğŸš€ ì‹¤ì œ DDPG í•™ìŠµ ì‹œì‘ (ID: {task_id})")

            # ì¥ì¹˜ ì„¤ì •
            device = self._get_device()
            self.add_log(f"ğŸ”§ ì—°ì‚° ì¥ì¹˜: {device}")

            # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„¤ì •
            env, agent = self._setup_training_environment(config, device)

            # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
            self._run_ddpg_training(env, agent, config, task_id)

        except Exception as e:
            error_msg = f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {task_id}): {str(e)}"
            self.add_log(error_msg)
            self.logger.error(error_msg, exc_info=True)
        finally:
            # í•™ìŠµ ì™„ë£Œ ì²˜ë¦¬
            self.is_training = False
            self.add_log(f"âœ… í•™ìŠµ ì™„ë£Œ (ID: {task_id})")

            # ìƒíƒœ ì½œë°±ì„ í†µí•´ DashManagerì— ì¢…ë£Œ ìƒíƒœ ì „ë‹¬
            if self.status_callback:
                self.status_callback(is_training=False, can_stop=False)

            # í•™ìŠµ ì™„ë£Œ ì‹œ í˜„ì¬ ìƒíƒœ ì´ˆê¸°í™”
            self.current_agent = None
            self.current_config = None
            self.current_episode = 0
            self.model_dir = None

    def _get_device(self):
        """ìµœì  ì¥ì¹˜ ì„ íƒ"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        else:
            return torch.device("cpu")

    def _setup_training_environment(
        self, config: Dict[str, Any], device
    ) -> Tuple[TradingEnvironment, DDPGAgent]:
        """í•™ìŠµ í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„¤ì •"""

        # ë°ì´í„° ë¡œë“œ
        self.add_log(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘... (ìì‚°: {', '.join(config['assets'])})")

        # ETF ì¡°í•©ì— ë”°ë¥¸ ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„±
        etf_combination = "_".join(sorted(config["assets"]))
        filename = f"rl_ddpg_{etf_combination}"

        merged_data = load_merged_data_v1(config["assets"], filename, refresh=False)
        self.add_log(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(merged_data)} í–‰)")

        # í™˜ê²½ ìƒì„±
        self.add_log("ğŸ—ï¸ íŠ¸ë ˆì´ë”© í™˜ê²½ ì„¤ì • ì¤‘...")
        env = TradingEnvironment(
            merged_data,
            self.logger,
            window_size=config["window_size"],
            n_assets=len(config["assets"]),
        )

        # ì—ì´ì „íŠ¸ ìƒì„±
        self.add_log("ğŸ¤– DDPG ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        state_dim = (
            env.observation_space.shape[0] if env.observation_space is not None else 0
        )
        action_dim = env.action_space.shape[0] if env.action_space is not None else 0

        agent = DDPGAgent(
            self.logger,
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=config["hidden_dim"],
            actor_lr=config["actor_lr"],
            critic_lr=config["critic_lr"],
            device=str(device),
            critic_loss_type=config.get("critic_loss_type", "mse"),
        )

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        model_dir = f"./model/rl_ddpg_{self.task_id}"
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            self.add_log(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±: {model_dir}")

        self.add_log(
            f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘... (ì—í”¼ì†Œë“œ: {config['episodes_resume']})"
        )
        agent.load_checkpoint(model_dir, config["episodes_resume"])

        return env, agent

    def _run_ddpg_training(self, env, agent, config: Dict[str, Any], task_id: str):
        """ì‹¤ì œ DDPG í•™ìŠµ ì‹¤í–‰"""

        # í˜„ì¬ í•™ìŠµ ìƒíƒœ ì €ì¥ (ìˆ˜ë™ ì €ì¥ìš©)
        self.current_agent = agent
        self.current_config = config
        self.model_dir = f"./model/rl_ddpg_{self.task_id}"

        self.total_episodes = config["episodes"]
        episodes_resume = config["episodes_resume"]
        batch_size = config["batch_size"]
        episodes_save = config["episodes_save"]

        # ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™”
        replay_buffer = deque(maxlen=100000)
        self.add_log(f"ğŸ’¾ ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™” ì™„ë£Œ (ìµœëŒ€ í¬ê¸°: 100,000)")

        # ê¸°ë³¸ ì•¡ì…˜ë“¤
        action_equal = np.ones(env.action_space.shape) / env.action_space.shape[0]
        action_balance = np.zeros(env.action_space.shape)
        action_balance[env.action_space.shape[0] - 1] = 1

        # í‰ê·  ë³´ìƒ ê³„ì‚°ìš©
        reward_window = deque(maxlen=100)

        total_start_time = time.time()
        update = 0

        self.add_log(
            f"ğŸ¯ í•™ìŠµ ì‹œì‘! ëª©í‘œ: {self.total_episodes} ì—í”¼ì†Œë“œ, ë°°ì¹˜ í¬ê¸°: {batch_size}"
        )

        # ì—í”¼ì†Œë“œ ë£¨í”„
        for episode in range(episodes_resume, self.total_episodes):
            # ğŸ›‘ ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
            if self.stop_event and self.stop_event.is_set():
                self.add_log(
                    f"ğŸ›‘ ì¤‘ì§€ ì‹ í˜¸ ê°ì§€ - ì—í”¼ì†Œë“œ {episode}ì—ì„œ ì•ˆì „í•˜ê²Œ ì¤‘ë‹¨"
                )
                break

            episode_start_time = time.time()
            self.current_episode = episode

            # ë§¤ ì—í”¼ì†Œë“œ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì°¨íŠ¸ ì‹¤ì‹œê°„ ê°±ì‹ ìš©)
            progress_percent = (episode / self.total_episodes) * 100
            self.update_status(
                episode=episode,
                total_episodes=self.total_episodes,
                progress=progress_percent,
                current_reward=getattr(self, "current_reward", 0),
                average_reward=getattr(self, "average_reward", 0),
                portfolio_value=getattr(self, "portfolio_value", 0),
                actor_loss=getattr(self, "actor_loss", 0),
                critic_loss=getattr(self, "critic_loss", 0),
                update_count=getattr(self, "update_count", 0),
            )

            # ì—í”¼ì†Œë“œ ì‹œì‘ ë¡œê·¸
            if episode % 5 == 0 or episode < 3:
                progress_percent = (episode / self.total_episodes) * 100
                self.add_log(
                    f"ğŸ“ˆ ì—í”¼ì†Œë“œ {episode}/{self.total_episodes} ì‹œì‘ (ì§„í–‰ë¥ : {progress_percent:.1f}%)"
                )

            state = env.reset(episode)
            episode_reward = 0
            step_count = 0

            # ê²€ì¦ ìƒíƒœ ì¶”ì 
            action_verification = None
            state_verification = None
            state_next_verification = None

            count_monthly_agent = 0
            count_monthly_equal = 0
            count_monthly_balance = 0

            # ì—í”¼ì†Œë“œ ë‚´ ìŠ¤í… ë£¨í”„ ì‹¤í–‰
            episode_reward, step_count, update = self._run_episode_steps(
                env,
                agent,
                replay_buffer,
                batch_size,
                episode,
                episode_reward,
                step_count,
                update,
                reward_window,
            )

            # ğŸ›‘ ì—í”¼ì†Œë“œ ì™„ë£Œ í›„ì—ë„ ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
            if self.stop_event and self.stop_event.is_set():
                self.add_log(f"ğŸ›‘ ì—í”¼ì†Œë“œ {episode} ì™„ë£Œ í›„ ì¤‘ì§€ ì‹ í˜¸ ê°ì§€")
                break

            # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬
            self._process_episode_completion(
                env, episode, episode_reward, episode_start_time, step_count
            )

            # ì •ê¸° ëª¨ë¸ ì €ì¥ ì²˜ë¦¬
            if episode % config.get("episodes_save", 10) == 0:
                self._save_model_checkpoint(
                    episode, config, total_start_time, reward_window
                )

        total_elapsed_time = time.time() - total_start_time

        # í•™ìŠµ ì™„ë£Œ/ì¤‘ë‹¨ ë©”ì‹œì§€
        if self.stop_event and self.stop_event.is_set():
            completion_message = f"â¹ï¸ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (ì—í”¼ì†Œë“œ {self.current_episode}/{self.total_episodes}). ì´ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ"
        else:
            completion_message = f"ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì´ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ"

        self.add_log(completion_message)

    def _run_episode_steps(
        self,
        env,
        agent,
        replay_buffer,
        batch_size,
        episode,
        episode_reward,
        step_count,
        update,
        reward_window,
    ):
        """ì—í”¼ì†Œë“œ ë‚´ ìŠ¤í…ë“¤ì„ ì‹¤í–‰"""

        # ê²€ì¦ ìƒíƒœ ì¶”ì 
        action_verification = None
        state_verification = None
        state_next_verification = None

        count_monthly_agent = 0
        count_monthly_equal = 0

        state = env.reset(episode)

        while True:
            # ğŸ›‘ ë§¤ ìŠ¤í…ë§ˆë‹¤ ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
            if self.stop_event and self.stop_event.is_set():
                self.add_log(f"ğŸ›‘ ì—í”¼ì†Œë“œ {episode} ì¤‘ê°„ì— ì¤‘ì§€ ì‹ í˜¸ ê°ì§€")
                break

            step_count += 1

            # ì•¡ì…˜ ì„ íƒ
            # select_actionì—ì„œ ì´ë¯¸ ìµœì†Œ ë¹„ì¤‘ 7.5% ì œì•½ì„ ë³´ì¥í•˜ë¯€ë¡œ ì¶”ê°€ í´ë¦¬í•‘ ë¶ˆí•„ìš”
            action, raw_action = agent.select_action(state)

            # ì•¡ì…˜ ìœ íš¨ì„± ê²€ì‚¬ (ìµœì†Œ ë¹„ì¤‘ ì œì•½ í™•ì¸)
            # select_actionì—ì„œ ì´ë¯¸ ìµœì†Œ ë¹„ì¤‘ 7.5%ë¥¼ ë³´ì¥í•˜ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ê²€ì¦
            min_weight = 0.075
            if np.min(action) < min_weight:
                # ìµœì†Œ ë¹„ì¤‘ ë¯¸ë§Œì¸ ê²½ìš° ì¬ì¡°ì • (ì´ë¡ ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨)
                action = np.maximum(action, min_weight)
                action = action / np.sum(action)

            # í•©ì´ 1ì¸ì§€ í™•ì¸ (ì´ë¡ ì ìœ¼ë¡œ ì´ë¯¸ ë³´ì¥ë¨)
            action_sum = np.sum(action)
            if abs(action_sum - 1.0) > 1e-6:
                action = action / action_sum

            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            (
                next_state,
                reward_agent,
                reward_monthly_agent,
                reward_monthly_equal,
                done,
                _,
                verification,
            ) = env.step(action)

            # ê²€ì¦ëœ ìŠ¤í…ì¸ ê²½ìš° ì²˜ë¦¬
            if verification:
                episode_reward += reward_monthly_agent

                # ë¦¬í”Œë ˆì´ ë²„í¼ì— ê²½í—˜ ì €ì¥ (ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ì„ íƒí•œ ì•¡ì…˜ë§Œ)
                if (
                    action_verification is not None
                    and state_verification is not None
                    and state_next_verification is not None
                ):
                    replay_buffer.append(
                        (
                            state_verification,
                            action_verification,
                            reward_monthly_agent,
                            state_next_verification,
                            float(done),
                        )
                    )

                    # ìµœê³  ì„±ê³¼ ì¶”ì  (ë¹„êµ ëª©ì )
                    max_reward = max(reward_monthly_agent, reward_monthly_equal)
                    if max_reward == reward_monthly_agent:
                        count_monthly_agent += 1
                    elif max_reward == reward_monthly_equal:
                        count_monthly_equal += 1

                action_verification = action
                state_verification = state
                state_next_verification = next_state

            # í•™ìŠµ ì—…ë°ì´íŠ¸
            if len(replay_buffer) >= batch_size:
                # ğŸ›‘ í•™ìŠµ ì „ì—ë„ ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
                if self.stop_event and self.stop_event.is_set():
                    self.add_log("ğŸ›‘ í•™ìŠµ ì—…ë°ì´íŠ¸ ì¤‘ ì¤‘ì§€ ì‹ í˜¸ ê°ì§€")
                    break

                batch = random.sample(replay_buffer, batch_size)
                self.actor_loss, self.critic_loss = agent.update(batch)
                update += 1
                self.update_count = update

                # ì£¼ê¸°ì ìœ¼ë¡œ ì†ì‹¤ ê°’ ë¡œê·¸ ë° ì°¨íŠ¸ ì—…ë°ì´íŠ¸
                if update % 100 == 0:  # ë” ìì£¼ ì—…ë°ì´íŠ¸ (1000 â†’ 100)
                    env.render()

                    # ì¤‘ê°„ ì—…ë°ì´íŠ¸ ì‹œì—ë„ ìƒíƒœ ì „ì†¡
                    self.update_status(
                        episode=episode,
                        total_episodes=self.total_episodes,
                        progress=(episode / self.total_episodes) * 100,
                        current_reward=episode_reward,
                        average_reward=np.mean(reward_window) if reward_window else 0,
                        portfolio_value=env._calculate_value(),
                        actor_loss=self.actor_loss,
                        critic_loss=self.critic_loss,
                        update_count=update,
                        is_training=True,
                        mid_episode_update=True,  # ì¤‘ê°„ ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
                    )

                    self.add_log(
                        f"ğŸ”„ í•™ìŠµ ì§„í–‰ - ì—…ë°ì´íŠ¸: {update}, Actor Loss: {self.actor_loss:.4f}, Critic Loss: {self.critic_loss:.4f}"
                    )

            state = next_state

            if done:
                break

            # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ëŒ€ê¸°
            if step_count % 100 == 0:
                time.sleep(0.001)

        return episode_reward, step_count, update

    def _process_episode_completion(
        self, env, episode, episode_reward, episode_start_time, step_count
    ):
        """ì—í”¼ì†Œë“œ ì™„ë£Œ í›„ ì²˜ë¦¬"""

        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬
        final_portfolio_value = env._calculate_value()
        self.portfolio_value = final_portfolio_value

        reward_window = deque(
            maxlen=100
        )  # ì„ì‹œë¡œ ìƒì„±, ì‹¤ì œë¡œëŠ” ë©”ì„œë“œ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì•¼ í•¨
        reward_window.append(episode_reward)
        self.current_reward = episode_reward
        self.average_reward = np.mean(reward_window)

        elapsed_time = time.time() - episode_start_time

        # ì—í”¼ì†Œë“œ ì‹œê°„ ì¶”ì 
        self.episode_times.append(elapsed_time)

        # ì´ í•™ìŠµ ì‹œê°„ ê³„ì‚°
        if self.training_start_time:
            self.total_training_time = time.time() - self.training_start_time

        # ì—í”¼ì†Œë“œ ì™„ë£Œ í›„ ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸ (í’ë¶€í•œ ë°ì´í„°)
        self.update_status(
            episode=episode,
            total_episodes=self.total_episodes,
            progress=(episode / self.total_episodes) * 100,
            current_reward=episode_reward,
            average_reward=self.average_reward,
            portfolio_value=final_portfolio_value,
            actor_loss=getattr(self, "actor_loss", 0),
            critic_loss=getattr(self, "critic_loss", 0),
            update_count=getattr(self, "update_count", 0),
            episode_time=elapsed_time,
            step_count=step_count,
            is_training=True,
        )

        # ë¡œê·¸ ë©”ì‹œì§€ ì „ì†¡
        if episode % 3 == 0 or episode < 5:
            log_message = (
                f"âœ… ì—í”¼ì†Œë“œ {episode} ì™„ë£Œ: ë³´ìƒ {episode_reward:.2f}, "
                f"í¬íŠ¸í´ë¦¬ì˜¤ ${final_portfolio_value:.2f}, "
                f"ì‹œê°„ {elapsed_time:.1f}ì´ˆ, ìŠ¤í… {step_count}ê°œ"
            )
            self.add_log(log_message)

        # ìƒì„¸ ì„±ê³¼ ë¡œê·¸
        if episode % 10 == 0 and episode > 0:
            performance_ratio = (
                final_portfolio_value / env.total_invested
                if env.total_invested > 0
                else 1.0
            )
            detailed_log = (
                f"ğŸ“Š ìƒì„¸ ì„±ê³¼ (ì—í”¼ì†Œë“œ {episode}) - "
                f"íˆ¬ìê¸ˆ: ${env.total_invested:.2f}, "
                f"ìˆ˜ìµë¥ : {((performance_ratio - 1) * 100):.2f}%"
            )
            self.add_log(detailed_log)

    def _save_model_checkpoint(self, episode, config, total_start_time, reward_window):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""

        self.add_log(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘ - ì—í”¼ì†Œë“œ {episode}")

        # í˜„ì¬ í•™ìŠµ ì„¤ì •ì„ ë©”íƒ€ë°ì´í„°ë¡œ ì „ë‹¬
        current_config = config.copy()

        # í•™ìŠµ ì‹œê°„ í†µê³„ ê³„ì‚°
        avg_episode_time = np.mean(self.episode_times) if self.episode_times else 0.0
        remaining_episodes = self.total_episodes - episode
        estimated_time_remaining = avg_episode_time * remaining_episodes

        current_config.update(
            {
                "current_episode": episode,
                "total_episodes": self.total_episodes,
                "task_id": self.task_id,
                "average_reward": np.mean(reward_window) if reward_window else 0.0,
                "training_start_time": total_start_time,
                # ETF ì •ë³´ ì¶”ê°€
                "selected_etfs": config.get("assets", []),
                "etf_count": len(config.get("assets", [])),
                # DDPG ì•Œê³ ë¦¬ì¦˜ ì„¤ì • ì¶”ê°€
                "max_grad_norm": config.get("max_grad_norm", 0.5),
                "critic_loss_type": config.get("critic_loss_type", "mse"),
                # ìƒˆë¡œìš´ í•™ìŠµ ì‹œê°„ ë©”íƒ€ë°ì´í„°
                "total_training_time_hours": self.total_training_time / 3600.0,
                "average_episode_time_seconds": float(avg_episode_time),
                "estimated_remaining_time_hours": estimated_time_remaining / 3600.0,
                "completed_episodes_count": len(self.episode_times),
                "training_efficiency_episodes_per_hour": (
                    len(self.episode_times) / (self.total_training_time / 3600.0)
                    if self.total_training_time > 0
                    else 0.0
                ),
            }
        )

        start_save_time = time.time()

        # ë©”ì¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ì— ì €ì¥
        model_dir = f"./model/rl_ddpg_{self.task_id}"
        os.makedirs(model_dir, exist_ok=True)
        if self.current_agent:
            self.current_agent.save_checkpoint(model_dir, episode, current_config)

        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì—ë„ ì €ì¥
        latest_model_dir = "./model/rl_ddpg_latest"
        os.makedirs(latest_model_dir, exist_ok=True)
        if self.current_agent:
            self.current_agent.save_checkpoint(
                latest_model_dir, episode, current_config
            )

        save_time = time.time() - start_save_time
        self.add_log(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {save_time:.2f}ì´ˆ)")
        self.add_log(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {model_dir} ë° {latest_model_dir}")

        # ëª¨ë¸ ì €ì¥ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.update_status(
            episode=episode,
            total_episodes=self.total_episodes,
            progress=(episode / self.total_episodes) * 100,
            current_reward=self.current_reward,
            average_reward=self.average_reward,
            portfolio_value=self.portfolio_value,
            actor_loss=self.actor_loss,
            critic_loss=self.critic_loss,
            update_count=self.update_count,
            is_training=True,
        )

    def manual_save_model(self) -> bool:
        """ìˆ˜ë™ìœ¼ë¡œ í˜„ì¬ ëª¨ë¸ ì €ì¥"""
        if not self.is_training or not self.current_agent or not self.model_dir:
            self.add_log("âŒ í•™ìŠµ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return False

        try:
            # í˜„ì¬ í•™ìŠµ ì„¤ì •ì„ ë©”íƒ€ë°ì´í„°ë¡œ êµ¬ì„±
            current_config = self.current_config.copy() if self.current_config else {}

            # í•™ìŠµ ì‹œê°„ í†µê³„ ê³„ì‚°
            avg_episode_time = (
                np.mean(self.episode_times) if self.episode_times else 0.0
            )
            remaining_episodes = self.total_episodes - self.current_episode
            estimated_time_remaining = avg_episode_time * remaining_episodes

            current_config.update(
                {
                    "current_episode": self.current_episode,
                    "total_episodes": self.total_episodes,
                    "task_id": self.task_id,
                    "manual_save": True,  # ìˆ˜ë™ ì €ì¥ í‘œì‹œ
                    "save_time": time.time(),
                    # DDPG ì•Œê³ ë¦¬ì¦˜ ì„¤ì • ì¶”ê°€
                    "max_grad_norm": (
                        self.current_config.get("max_grad_norm", 0.5)
                        if self.current_config
                        else 0.5
                    ),
                    "critic_loss_type": (
                        self.current_config.get("critic_loss_type", "mse")
                        if self.current_config
                        else "mse"
                    ),
                    # í•™ìŠµ ì‹œê°„ ë©”íƒ€ë°ì´í„°
                    "total_training_time_hours": self.total_training_time / 3600.0,
                    "average_episode_time_seconds": float(avg_episode_time),
                    "estimated_remaining_time_hours": estimated_time_remaining / 3600.0,
                    "completed_episodes_count": len(self.episode_times),
                    "training_efficiency_episodes_per_hour": (
                        len(self.episode_times) / (self.total_training_time / 3600.0)
                        if self.total_training_time > 0
                        else 0.0
                    ),
                }
            )

            # ë©”ì¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ì— ì €ì¥
            self.current_agent.save_checkpoint(
                self.model_dir, self.current_episode, current_config
            )

            # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì—ë„ ì €ì¥
            latest_model_dir = "./model/rl_ddpg_latest"
            os.makedirs(latest_model_dir, exist_ok=True)
            self.current_agent.save_checkpoint(
                latest_model_dir, self.current_episode, current_config
            )

            # ëª¨ë¸ í•´ì‹œ ê³„ì‚° ë° ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            model_hash = calculate_model_hash(self.model_dir)
            current_config["model_hash"] = model_hash
            current_config["model_integrity_check"] = (
                True
                if model_hash != "no_model_files"
                and not model_hash.startswith("hash_error")
                else False
            )

            # í•´ì‹œê°€ í¬í•¨ëœ ë©”íƒ€ë°ì´í„°ë¡œ ë‹¤ì‹œ ì €ì¥ (ë‘ ê³³ ëª¨ë‘)
            self.current_agent.save_checkpoint(
                self.model_dir, self.current_episode, current_config
            )
            self.current_agent.save_checkpoint(
                latest_model_dir, self.current_episode, current_config
            )

            # ìƒì„¸ ì €ì¥ ë¡œê·¸
            self.add_log(f"ğŸ’¾ ìˆ˜ë™ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ì—í”¼ì†Œë“œ {self.current_episode}")
            self.add_log(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.model_dir} ë° {latest_model_dir}")
            self.add_log(
                f"ğŸ“Š í˜„ì¬ í•™ìŠµ ì‹œê°„: {self.total_training_time/3600.0:.2f}ì‹œê°„"
            )
            self.add_log(
                f"ğŸ” ëª¨ë¸ í•´ì‹œ: {model_hash[:12]}... (ë¬´ê²°ì„±: {'âœ…' if current_config['model_integrity_check'] else 'âŒ'})"
            )
            return True

        except Exception as e:
            error_msg = f"âŒ ìˆ˜ë™ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            self.add_log(error_msg)
            self.logger.error(error_msg, exc_info=True)
            return False
