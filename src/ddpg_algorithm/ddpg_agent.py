"""
DDPG (Deep Deterministic Policy Gradient) ì—ì´ì „íŠ¸
ê°•í™”í•™ìŠµ ê¸°ë°˜ ì—°ì†ì  í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ì—ì´ì „íŠ¸
"""

import os
import time
import json
from typing import Tuple, Optional, Dict, Any, List, Union
import logging

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from .ddpg_models import Actor, Critic
from .ddpg_noise import OUNoise


class DDPGAgent:
    """
    DDPG ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
    Deep Deterministic Policy Gradient ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”
    """

    def __init__(
        self,
        logger: logging.Logger,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 256,
        actor_lr: float = 3e-4,
        critic_lr: float = 3e-4,
        device: str = "cpu",
        max_grad_norm: float = 0.5,
        critic_loss_type: str = "mse",
    ):
        """
        Args:
            logger: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
            state_dim: ìƒíƒœ ê³µê°„ ì°¨ì›
            action_dim: í–‰ë™ ê³µê°„ ì°¨ì›
            hidden_dim: ì€ë‹‰ì¸µ ì°¨ì›
            actor_lr: Actor ë„¤íŠ¸ì›Œí¬ í•™ìŠµë¥ 
            critic_lr: Critic ë„¤íŠ¸ì›Œí¬ í•™ìŠµë¥ 
            device: ì—°ì‚° ì¥ì¹˜ ('cpu', 'cuda', 'mps')
            max_grad_norm: Gradient clipping ìµœëŒ€ norm ê°’ (ê¸°ë³¸ê°’: 0.5)
            critic_loss_type: Critic loss í•¨ìˆ˜ íƒ€ì… ('mse' ë˜ëŠ” 'smooth_l1', ê¸°ë³¸ê°’: 'mse')
        """
        self.actor = Actor(state_dim, action_dim).to(device)
        self.actor_target = Actor(state_dim, action_dim).to(device)
        self.actor_optimizer = optim.Adam(
            self.actor.parameters(), lr=actor_lr, weight_decay=1e-4
        )

        self.critic = Critic(state_dim, action_dim).to(device)
        self.critic_target = Critic(state_dim, action_dim).to(device)
        self.critic_optimizer = optim.Adam(
            self.critic.parameters(), lr=critic_lr, weight_decay=1e-4
        )

        self.gamma = 0.99  # í• ì¸ ì¸ìˆ˜
        self.tau = 0.001  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ë¹„ìœ¨ ì¡°ì •
        self.device = device
        self.logger = logger
        self.max_grad_norm = max_grad_norm  # Gradient clipping ìµœëŒ€ norm
        self.critic_loss_type = critic_loss_type  # Critic loss í•¨ìˆ˜ íƒ€ì…

        self.noise = OUNoise(action_dim, theta=0.15, sigma=0.2)  # ë…¸ì´ì¦ˆ íŒŒë¼ë¯¸í„° ì¡°ì •
        self.update_counter = 0
        self.update_freq = 2  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°

    def select_action(
        self, state: np.ndarray, add_noise: bool = True
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ìƒíƒœì— ë”°ë¥¸ í–‰ë™ ì„ íƒ
        ìµœì†Œ ë¹„ì¤‘ 10% ì œì•½ì„ ìœ ì§€í•˜ë©´ì„œ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€

        Args:
            state: í˜„ì¬ ìƒíƒœ
            add_noise: íƒí—˜ ë…¸ì´ì¦ˆ ì¶”ê°€ ì—¬ë¶€ (í•™ìŠµ ì‹œ True, í‰ê°€ ì‹œ False)

        Returns:
            (í´ë¦¬í•‘ëœ í–‰ë™, ì›ë³¸ í–‰ë™)
            - í´ë¦¬í•‘ëœ í–‰ë™: ìµœì†Œ ë¹„ì¤‘ 10% ì œì•½ì„ ë§Œì¡±í•˜ëŠ” ì•¡ì…˜
            - ì›ë³¸ í–‰ë™: Actor ë„¤íŠ¸ì›Œí¬ì˜ ì›ë³¸ ì¶œë ¥ (Affine Scaling ì ìš©ë¨)
        """
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            action = self.actor(state).squeeze(0)
        action = action.detach().cpu().numpy()
        raw_action = (
            action.copy()
        )  # ì›ë³¸ ì•¡ì…˜ ì €ì¥ (Affine Scaling ì ìš©ë¨, ì´ë¯¸ ìµœì†Œ 10% ë³´ì¥)

        if add_noise:
            # DDPG ë°©ì‹: OUNoiseë¥¼ ì‚¬ìš©í•œ íƒí—˜ (ì—°ì† í–‰ë™ ê³µê°„ì— ì í•©)
            # ì…ì‹¤ë¡  ê·¸ë¦¬ë””ëŠ” ì´ì‚° í–‰ë™ ê³µê°„ìš©ì´ë¯€ë¡œ DDPGì—ëŠ” ë¶€ì í•©
            action += self.noise.sample()  # íƒí—˜ì„ ìœ„í•œ ë…¸ì´ì¦ˆ ì¶”ê°€

            # ë…¸ì´ì¦ˆ ì¶”ê°€ í›„ ìµœì†Œ ë¹„ì¤‘ ì œì•½ì„ ìœ ì§€í•˜ëŠ” í´ë¦¬í•‘
            min_weight = 0.075  # ìµœì†Œ ë¹„ì¤‘ 7.5%

            # ê° ìš”ì†Œë¥¼ ìµœì†Œ ë¹„ì¤‘ ì´ìƒìœ¼ë¡œ í´ë¦¬í•‘
            action = np.maximum(action, min_weight)

            # í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
            action_sum = np.sum(action)
            if action_sum > 0:
                action = action / action_sum
            else:
                # ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš° ê· ë“± ë¶„ë°° (ê° ìš”ì†ŒëŠ” ìµœì†Œ 7.5% ì´ìƒ)
                action = np.ones_like(action) / len(action)

            # ì •ê·œí™” í›„ì—ë„ ìµœì†Œ ë¹„ì¤‘ ì œì•½ì´ ìœ ì§€ë˜ëŠ”ì§€ ê²€ì¦
            # (ì •ê·œí™”ë¡œ ì¸í•´ ì¼ë¶€ ìš”ì†Œê°€ 0.075 ë¯¸ë§Œìœ¼ë¡œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬ì¡°ì •)
            min_val = np.min(action)
            if min_val < min_weight:
                # ìµœì†Œê°’ì´ 0.075 ë¯¸ë§Œì¸ ê²½ìš°, ëª¨ë“  ìš”ì†Œë¥¼ ìµœì†Œ 0.075 ì´ìƒìœ¼ë¡œ ì¡°ì •
                action = np.maximum(action, min_weight)
                # ë‹¤ì‹œ ì •ê·œí™”
                action = action / np.sum(action)
        else:
            # ë…¸ì´ì¦ˆ ì—†ëŠ” ê²½ìš°: Actor ì¶œë ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ Affine Scalingìœ¼ë¡œ ìµœì†Œ 10% ë³´ì¥ë¨)
            # ì¶”ê°€ í´ë¦¬í•‘ ë¶ˆí•„ìš”
            pass

        return action, raw_action  # ìµœì¢… ì•¡ì…˜ê³¼ ì›ë³¸ ì•¡ì…˜ ë°˜í™˜

    def update(self, batch: List[Tuple]) -> Tuple[float, float]:
        """
        ë°°ì¹˜ ë°ì´í„°ë¡œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸

        Args:
            batch: (ìƒíƒœ, í–‰ë™, ë³´ìƒ, ë‹¤ìŒìƒíƒœ, ì™„ë£Œ) íŠœí”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸

        Returns:
            (actor ì†ì‹¤, critic ì†ì‹¤)
        """
        batch = list(zip(*batch))
        states, actions, rewards, next_states, dones = batch

        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.FloatTensor(np.array(actions)).to(self.device)
        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(self.device)

        # Critic update
        next_actions = self.actor_target(next_states)
        target_q = self.critic_target(next_states, next_actions)
        target_q = rewards + (1 - dones) * self.gamma * target_q
        current_q = self.critic(states, actions)

        # Critic loss ê³„ì‚° (loss íƒ€ì…ì— ë”°ë¼ ì„ íƒ)
        if self.critic_loss_type == "smooth_l1":
            critic_loss = F.smooth_l1_loss(current_q, target_q.detach())
        else:  # ê¸°ë³¸ê°’: mse
            critic_loss = F.mse_loss(current_q, target_q.detach())

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        # Gradient clipping ì ìš©
        nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
        self.critic_optimizer.step()

        # Actor update
        actor_loss = -self.critic(states, self.actor(states)).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        # Gradient clipping ì ìš©
        nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
        self.actor_optimizer.step()

        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸
        self._soft_update(self.actor_target, self.actor)
        self._soft_update(self.critic_target, self.critic)

        return actor_loss.item(), critic_loss.item()

    def _soft_update(self, target: torch.nn.Module, source: torch.nn.Module) -> None:
        """
        íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸

        Args:
            target: íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬
            source: ì†ŒìŠ¤ ë„¤íŠ¸ì›Œí¬
        """
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                target_param.data * (1.0 - self.tau) + param.data * self.tau
            )

    def save_checkpoint(
        self,
        model_dir: str,
        episode: int,
        training_config: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥

        Args:
            model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            episode: í˜„ì¬ ì—í”¼ì†Œë“œ
            training_config: í•™ìŠµ ì„¤ì • ë©”íƒ€ë°ì´í„°
        """
        checkpoint_name = os.path.join(model_dir, f"checkpoint_{episode:04d}.pth")
        checkpoint_last = os.path.join(model_dir, f"checkpoint_last.pth")

        # í•™ìŠµ ì„¤ì • ë©”íƒ€ë°ì´í„° ì¶”ê°€
        checkpoint_data = {
            "actor_state_dict": self.actor.state_dict(),
            "actor_target_state_dict": self.actor_target.state_dict(),
            "critic_state_dict": self.critic.state_dict(),
            "critic_target_state_dict": self.critic_target.state_dict(),
            "actor_optimizer_state_dict": self.actor_optimizer.state_dict(),
            "critic_optimizer_state_dict": self.critic_optimizer.state_dict(),
            "episode": episode,
            "save_time": time.time(),
            "training_metadata": training_config or {},
        }

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        torch.save(checkpoint_data, checkpoint_name)
        torch.save(checkpoint_data, checkpoint_last)

        # ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ë¡œë„ ë³„ë„ ì €ì¥ (ê°€ë…ì„±ì„ ìœ„í•´)
        if training_config:
            metadata_file = os.path.join(model_dir, f"metadata_{episode:04d}.json")
            metadata_last = os.path.join(model_dir, "metadata_last.json")

            metadata = {
                "episode": episode,
                "save_time": checkpoint_data["save_time"],
                "save_datetime": time.strftime(
                    "%Y-%m-%d %H:%M:%S", time.localtime(checkpoint_data["save_time"])
                ),
                "training_config": training_config,
                "model_info": {
                    "hidden_dim": training_config.get("hidden_dim", 256),
                    "learning_rates": {
                        "actor_lr": training_config.get("actor_lr", 0.0003),
                        "critic_lr": training_config.get("critic_lr", 0.0003),
                    },
                    "critic_loss_type": training_config.get("critic_loss_type", "mse"),
                    "max_grad_norm": training_config.get("max_grad_norm", 0.5),
                    "batch_size": training_config.get("batch_size", 128),
                    "window_size": training_config.get("window_size", 60),
                    "assets": training_config.get("assets", []),
                },
            }

            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            with open(metadata_last, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            self.logger.info(f"ğŸ’¾ ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {checkpoint_name}")

    def load_checkpoint(
        self, model_dir: str, episode: Optional[int] = None, evaluate: bool = False
    ) -> bool:
        """
        ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

        Args:
            model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            episode: ë¡œë“œí•  ì—í”¼ì†Œë“œ (Noneì‹œ ìµœì‹ )
            evaluate: í‰ê°€ ëª¨ë“œ ì—¬ë¶€

        Returns:
            ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        if episode is None:
            checkpoint_name = os.path.join(model_dir, "checkpoint_last.pth")
        else:
            checkpoint_name = os.path.join(model_dir, f"checkpoint_{episode:04d}.pth")
            if not os.path.exists(checkpoint_name):
                self.logger.info(f"Checkpoint {checkpoint_name} does not exist")
                checkpoint_name = os.path.join(model_dir, "checkpoint_last.pth")
                if not os.path.exists(checkpoint_name):
                    self.logger.info(f"Checkpoint {checkpoint_name} does not exist")
                    return False  # ëª…ì‹œì ìœ¼ë¡œ False ë°˜í™˜

        if checkpoint_name is not None and os.path.exists(checkpoint_name):
            try:
                self.logger.info("Loading models from {}".format(checkpoint_name))
                # PyTorch 2.6 í˜¸í™˜ì„±ì„ ìœ„í•´ weights_only=False ëª…ì‹œì  ì„¤ì •
                checkpoint = torch.load(
                    checkpoint_name, map_location=self.device, weights_only=False
                )
                self.logger.info(f"checkpoint: {checkpoint_name}")

                # ë©”ì¸ ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
                self.actor.load_state_dict(checkpoint["actor_state_dict"])
                self.critic.load_state_dict(checkpoint["critic_state_dict"])

                # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
                self.actor_target.load_state_dict(checkpoint["actor_target_state_dict"])
                self.critic_target.load_state_dict(
                    checkpoint["critic_target_state_dict"]
                )

                # ì˜µí‹°ë§ˆì´ì € ë¡œë“œ (í•™ìŠµ ì¬ê°œì‹œì—ë§Œ í•„ìš”)
                if not evaluate:
                    self.actor_optimizer.load_state_dict(
                        checkpoint["actor_optimizer_state_dict"]
                    )
                    self.critic_optimizer.load_state_dict(
                        checkpoint["critic_optimizer_state_dict"]
                    )

                # í‰ê°€ ëª¨ë“œ ì„¤ì •
                if evaluate:
                    self.actor.eval()
                    self.critic.eval()
                    self.actor_target.eval()
                    self.critic_target.eval()

                    # í‰ê°€ ì‹œ ë…¸ì´ì¦ˆ ìƒíƒœ ë¦¬ì…‹
                    self.noise.reset()
                    self.logger.info("ğŸ”• í‰ê°€ ëª¨ë“œ: ë…¸ì´ì¦ˆ ìƒíƒœ ë¦¬ì…‹ ì™„ë£Œ")

                    # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¥¼ ë©”ì¸ ë„¤íŠ¸ì›Œí¬ì™€ ì™„ì „ ë™ê¸°í™” (í‰ê°€ìš©)
                    self.actor_target.load_state_dict(self.actor.state_dict())
                    self.critic_target.load_state_dict(self.critic.state_dict())
                    self.logger.info("ğŸ”„ í‰ê°€ ëª¨ë“œ: íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ë™ê¸°í™” ì™„ë£Œ")
                else:
                    self.actor.train()
                    self.critic.train()
                    self.actor_target.train()
                    self.critic_target.train()

                # ë¡œë”©ëœ ëª¨ë¸ì˜ ì•¡ì…˜ ë¶„í¬ ê²€ì¦ (í‰ê°€ ëª¨ë“œì—ì„œë§Œ)
                if evaluate:
                    self._verify_model_diversity()

                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {checkpoint_name}")
                return True  # ì„±ê³µ ì‹œ True ë°˜í™˜
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False  # ì˜ˆì™¸ ë°œìƒ ì‹œ False ë°˜í™˜
        else:
            self.logger.error(
                f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_name}"
            )
            return False  # íŒŒì¼ì´ ì—†ìœ¼ë©´ False ë°˜í™˜

    def _verify_model_diversity(self) -> None:
        """ëª¨ë¸ì˜ ì•¡ì…˜ ë‹¤ì–‘ì„± ê²€ì¦ (í‰ê°€ ëª¨ë“œì—ì„œë§Œ ì‹¤í–‰)"""
        try:
            # ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ìƒíƒœì— ëŒ€í•´ ì•¡ì…˜ ë¶„í¬ í™•ì¸
            test_states = []
            for i in range(5):
                # ëœë¤ ìƒíƒœ ìƒì„± (ì‹¤ì œ í™˜ê²½ ìƒíƒœ ì°¨ì›ì— ë§ì¶°)
                test_state = np.random.randn(self.actor.fc1.in_features)  # ìƒíƒœ ì°¨ì›
                test_states.append(test_state)

            actions = []
            for state in test_states:
                action, _ = self.select_action(state, add_noise=False)
                actions.append(action)

            actions = np.array(actions)

            # ì•¡ì…˜ ë¶„ì‚°ì„± ê³„ì‚°
            action_std = np.std(actions, axis=0)
            action_mean = np.mean(actions, axis=0)

            self.logger.info(f"ğŸ¯ ì•¡ì…˜ ë‹¤ì–‘ì„± ê²€ì¦:")
            self.logger.info(f"   - í‰ê·  ì•¡ì…˜: {action_mean}")
            self.logger.info(f"   - ì•¡ì…˜ í‘œì¤€í¸ì°¨: {action_std}")
            self.logger.info(f"   - ë¶„ì‚°ì„± ì ìˆ˜: {np.mean(action_std):.4f}")

            # ë‚®ì€ ë¶„ì‚°ì„± ê²½ê³ 
            if np.mean(action_std) < 0.01:
                self.logger.warning(
                    "âš ï¸ ë‚®ì€ ì•¡ì…˜ ë¶„ì‚°ì„± ê°ì§€ - ëª¨ë¸ì´ í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³ìˆì„ ìˆ˜ ìˆìŒ"
                )

        except Exception as e:
            self.logger.warning(f"âš ï¸ ì•¡ì…˜ ë‹¤ì–‘ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")

    def reset_for_evaluation(self) -> None:
        """í‰ê°€ë¥¼ ìœ„í•œ ì—ì´ì „íŠ¸ ìƒíƒœ ë¦¬ì…‹"""
        self.noise.reset()
        self.actor.eval()
        self.critic.eval()
        self.actor_target.eval()
        self.critic_target.eval()
        self.logger.info("ğŸ”„ í‰ê°€ë¥¼ ìœ„í•œ ì—ì´ì „íŠ¸ ìƒíƒœ ë¦¬ì…‹ ì™„ë£Œ")
